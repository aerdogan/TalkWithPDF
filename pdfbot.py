import os
import streamlit as st
import google.generativeai as genai
from dotenv import load_dotenv
from PyPDF2 import PdfReader

# .env'den API anahtarÄ±nÄ± al
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
genai.configure(api_key=api_key)

# Gemini modelini baÅŸlat
model = genai.GenerativeModel("gemini-2.5-flash-lite")

st.set_page_config(page_title="Gemini PDF Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ“„ Gemini PDF Chatbot")

# PDF iÃ§eriÄŸini saklamak iÃ§in session state
if "pdf_text" not in st.session_state:
    st.session_state.pdf_text = ""

if "messages" not in st.session_state:
    st.session_state.messages = []

# PDF yÃ¼kleme alanÄ±
uploaded_file = st.file_uploader("Bir PDF yÃ¼kle:", type=["pdf"])
if uploaded_file is not None:
    reader = PdfReader(uploaded_file)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n"
    st.session_state.pdf_text = text
    st.success("PDF baÅŸarÄ±yla yÃ¼klendi âœ…")

# Daha Ã¶nceki mesajlarÄ± gÃ¶ster
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# KullanÄ±cÄ±dan soru al
if prompt := st.chat_input("Sorunu yazâ€¦"):
    # KullanÄ±cÄ± mesajÄ±nÄ± ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Gemini'den streaming cevap
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            # KullanÄ±cÄ±nÄ±n sorusunu PDF baÄŸlamÄ± ile birleÅŸtir
            context_prompt = f"""Sen bir yardÄ±mcÄ± asistansÄ±n.
AÅŸaÄŸÄ±da bir PDF iÃ§eriÄŸi var. KullanÄ±cÄ± sadece bu iÃ§erikle ilgili sorular sorabilir.
EÄŸer cevap PDF'de yoksa "PDF iÃ§eriÄŸinde buna dair bilgi yok" de.

PDF iÃ§eriÄŸi:
{st.session_state.pdf_text}

Soru:
{prompt}
"""
            # Streaming cevabÄ± Ã¼ret
            response = model.generate_content(context_prompt, stream=True)

            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)

        except Exception as e:
            full_response = f"Bir hata oluÅŸtu: {e}"
            message_placeholder.markdown(full_response)

        # Sohbet geÃ§miÅŸine kaydet
        st.session_state.messages.append({"role": "assistant", "content": full_response})
